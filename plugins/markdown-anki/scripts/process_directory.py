#!/usr/bin/env python3
"""
æ‰¹é‡ç›®å½•å¤„ç†é›†æˆè„šæœ¬

è‡ªåŠ¨å¤„ç†æ•´ä¸ªç›®å½•çš„ Markdown æ–‡ä»¶ï¼ŒåŒ…æ‹¬ï¼š
1. æ‰¹é‡æå–æ‰€æœ‰æ–‡ä»¶çš„ç”Ÿè¯ï¼ˆæ–‡ä»¶å†…å»é‡ï¼‰
2. æŸ¥è¯¢ç¼“å­˜ï¼ˆå…¨å±€å»é‡ï¼‰
3. åˆ†æ‰¹è¾“å‡ºéœ€è¦ç¿»è¯‘çš„å•è¯ï¼ˆæ¯æ‰¹æœ€å¤š30ä¸ªï¼‰
4. ä½¿ç”¨ Claude Code ç¿»è¯‘æ¯æ‰¹å•è¯
5. ä¿å­˜ç¿»è¯‘åˆ°ç¼“å­˜
6. ç”Ÿæˆåˆå¹¶çš„ Anki æ–‡ä»¶

ç¡®ä¿æ‰€æœ‰å•è¯åªç¿»è¯‘ä¸€æ¬¡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ã€‚
"""

import json
import sys
from pathlib import Path

from batch_extract import extract_words_from_directory
from translation_cache import TranslationCache
from generate_anki import generate_anki_tsv
from config import get_output_dir

# æ¯æ‰¹ç¿»è¯‘çš„æœ€å¤§å•è¯æ•°
BATCH_SIZE = 30


def process_directory(directory: str, output_file: str = None) -> dict:
    """
    å¤„ç†æ•´ä¸ªç›®å½•çš„ Markdown æ–‡ä»¶

    Args:
        directory: åŒ…å« Markdown æ–‡ä»¶çš„ç›®å½•
        output_file: è¾“å‡ºçš„ Anki æ–‡ä»¶å

    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    # 1. æ‰¹é‡æå–ç”Ÿè¯
    print(f"[1/5] æ‰¹é‡æå–ç”Ÿè¯ï¼š{directory}")
    all_data = extract_words_from_directory(directory, '/tmp')

    if not all_data:
        print("  æ²¡æœ‰æ‰¾åˆ°æ ‡è®°çš„ç”Ÿè¯ï¼Œé€€å‡º")
        return {'total': 0, 'cached': 0, 'new': 0}

    total_words = sum(data['word_count'] for data in all_data)
    print(f"  âœ“ ä» {len(all_data)} ä¸ªæ–‡ä»¶ä¸­æå–åˆ° {total_words} ä¸ªç”Ÿè¯ï¼ˆå·²å»é‡ï¼‰")

    # 2. æŸ¥è¯¢ç¼“å­˜å¹¶å…¨å±€å»é‡
    print("\n[2/5] æŸ¥è¯¢ç¿»è¯‘ç¼“å­˜å¹¶å…¨å±€å»é‡")
    cache = TranslationCache()

    # è·Ÿè¸ªæ‰€æœ‰å•è¯ï¼ˆå…¨å±€å»é‡ï¼‰
    global_seen_words = set()
    all_cached_words = []
    all_uncached_words = []

    for file_data in all_data:
        for word_item in file_data['words']:
            word = word_item['word_lower']

            # å…¨å±€å»é‡ï¼šå¦‚æœè¿™ä¸ªå•è¯åœ¨ä¹‹å‰çš„æ–‡ä»¶ä¸­å·²ç»å‡ºç°è¿‡ï¼Œè·³è¿‡
            if word in global_seen_words:
                continue

            global_seen_words.add(word)
            cached_translation = cache.get(word)

            if cached_translation:
                # ä½¿ç”¨ç¼“å­˜çš„ç¿»è¯‘
                word_item['translation'] = cached_translation['translation']
                examples = cached_translation.get('sentence_examples', [])
                if examples:
                    word_item['sentence_translation'] = examples[0]['sentence_translation']
                else:
                    word_item['sentence_translation'] = ''
                all_cached_words.append(word_item)
            else:
                # æ·»åŠ  deck_name ä¿¡æ¯
                word_item['deck_name'] = file_data['deck_name']
                all_uncached_words.append(word_item)

    print(f"  âœ“ å…¨å±€å»é‡åï¼š{len(global_seen_words)} ä¸ªå”¯ä¸€å•è¯")
    print(f"  âœ“ æ‰¾åˆ° {len(all_cached_words)} ä¸ªå·²ç¼“å­˜çš„å•è¯")
    print(f"  âœ“ éœ€è¦ç¿»è¯‘ {len(all_uncached_words)} ä¸ªæ–°å•è¯")

    # 3. è¾“å‡ºéœ€è¦ç¿»è¯‘çš„å•è¯ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    if all_uncached_words:
        print("\n[3/5] éœ€è¦ç¿»è¯‘çš„å•è¯åˆ—è¡¨ï¼š")
        print("â”€" * 60)

        dir_name = Path(directory).name

        # è®¡ç®—éœ€è¦åˆ†æˆå¤šå°‘æ‰¹
        total_batches = (len(all_uncached_words) + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"\n  æ€»å…± {len(all_uncached_words)} ä¸ªæ–°å•è¯ï¼Œå°†åˆ†æˆ {total_batches} æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹æœ€å¤š {BATCH_SIZE} ä¸ªï¼‰")

        # åˆ†æ‰¹ä¿å­˜å¾…ç¿»è¯‘å•è¯åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_files = []
        for batch_num in range(total_batches):
            start_idx = batch_num * BATCH_SIZE
            end_idx = min((batch_num + 1) * BATCH_SIZE, len(all_uncached_words))
            batch_words = all_uncached_words[start_idx:end_idx]

            # ç”Ÿæˆæ‰¹æ¬¡æ–‡ä»¶å
            if total_batches == 1:
                temp_file = Path('/tmp') / f"{dir_name}_to_translate.json"
            else:
                temp_file = Path('/tmp') / f"{dir_name}_to_translate_batch_{batch_num + 1}.json"

            batch_data = {
                'directory': directory,
                'batch_info': f"æ‰¹æ¬¡ {batch_num + 1}/{total_batches}",
                'words': batch_words
            }
            temp_file.write_text(
                json.dumps(batch_data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            temp_files.append(str(temp_file))

            print(f"\næ‰¹æ¬¡ {batch_num + 1}/{total_batches}ï¼š{len(batch_words)} ä¸ªå•è¯ -> {temp_file}")

        print("\n" + "â”€" * 60)
        print("\nğŸ“ ä½¿ç”¨ Claude Code ç¿»è¯‘å•è¯ï¼š")
        print("\nå¯¹äºæ¯ä¸ªæ‰¹æ¬¡æ–‡ä»¶ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š")
        print("\n1. åœ¨ Claude Code ä¸­è¾“å…¥ï¼š")
        print("   \"è¯·å¸®æˆ‘ç¿»è¯‘è¿™ä¸ªæ–‡ä»¶ä¸­çš„å•è¯ï¼ŒæŒ‰ç…§æ–‡ä»¶ä¸­æä¾›çš„æ ¼å¼è¿”å›ç¿»è¯‘ç»“æœ\"")
        print("\n2. å°†ç¿»è¯‘ç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼štranslation_batch_1.jsonï¼‰")
        print("\n3. è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¿å­˜ç¿»è¯‘ï¼š")
        for i, temp_file in enumerate(temp_files, 1):
            print(f"   python3 scripts/process_directory.py {directory} translation_batch_{i}.json")

        print("\n" + "â”€" * 60)
        print("\nç¿»è¯‘æ ¼å¼ç¤ºä¾‹ï¼ˆéœ€åˆ—å‡ºå•è¯çš„æ‰€æœ‰å¸¸ç”¨è¯ä¹‰ï¼Œä¸è¯å…¸ä¸€è‡´ï¼‰ï¼š")
        print('[')
        print('  {')
        print('    "word": "example",')
        print('    "translation": "n. ä¾‹å­ï¼›èŒƒä¾‹ï¼›æ¦œæ · v. ä½œä¸º...çš„ä¾‹å­",')
        print('    "sentence": "This is an example.",')
        print('    "sentence_translation": "è¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚"')
        print('  }')
        print(']')

        print("\nğŸ’¡ æç¤ºï¼š")
        print(f"  - æ¯æ‰¹æœ€å¤š {BATCH_SIZE} ä¸ªå•è¯ï¼Œç¡®ä¿ä¸è¶…è¿‡ Claude Code çš„ä¸Šä¸‹æ–‡é™åˆ¶")
        print("  - ç¿»è¯‘å®Œä¸€æ‰¹åå†å¤„ç†ä¸‹ä¸€æ‰¹")
        print("  - æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆåï¼Œä¼šè‡ªåŠ¨åˆå¹¶ç”Ÿæˆæœ€ç»ˆçš„ Anki æ–‡ä»¶")

        return {
            'total': total_words,
            'unique': len(global_seen_words),
            'cached': len(all_cached_words),
            'new': len(all_uncached_words),
            'temp_files': temp_files,
            'total_batches': total_batches
        }

    else:
        print("\n[3/5] æ‰€æœ‰å•è¯éƒ½å·²ç¼“å­˜ï¼Œæ— éœ€ç¿»è¯‘")

        # 4. é‡å»ºå®Œæ•´æ•°æ®ï¼ˆåŒ…å«ç¼“å­˜çš„ç¿»è¯‘ï¼‰
        print("\n[4/5] é‡å»ºå®Œæ•´æ•°æ®")

        # ä¸ºæ¯ä¸ªæ–‡ä»¶é‡æ–°å¡«å……ç¿»è¯‘
        for file_data in all_data:
            for word_item in file_data['words']:
                word = word_item['word_lower']
                cached_translation = cache.get(word)
                if cached_translation:
                    word_item['translation'] = cached_translation['translation']
                    examples = cached_translation.get('sentence_examples', [])
                    if examples:
                        word_item['sentence_translation'] = examples[0]['sentence_translation']

        # 5. ç”Ÿæˆ Anki æ–‡ä»¶
        print("\n[5/5] ç”Ÿæˆ Anki æ–‡ä»¶")

        if output_file is None:
            dir_name = Path(directory).name
            output_dir = get_output_dir()
            output_file = str(output_dir / f"{dir_name}.txt")

        generate_anki_tsv(all_data, output_file, deck_name=dir_name)
        print(f"  âœ“ å·²ç”Ÿæˆï¼š{output_file}")

        print("\nå®Œæˆï¼")
        print(f"  æ€»å•è¯æ•°ï¼š{total_words}")
        print(f"  å”¯ä¸€å•è¯ï¼š{len(global_seen_words)}")
        print(f"  ä½¿ç”¨ç¼“å­˜ï¼š{len(all_cached_words)}")
        print(f"  æ–°ç¿»è¯‘ï¼š0")

        return {
            'total': total_words,
            'unique': len(global_seen_words),
            'cached': len(all_cached_words),
            'new': 0,
            'output_file': output_file
        }


def save_and_generate(temp_file: str, translation_file: str, output_file: str = None):
    """
    ä¿å­˜ç¿»è¯‘å¹¶ç”Ÿæˆ Anki æ–‡ä»¶ï¼ˆæ”¯æŒå•æ‰¹å’Œå¤šæ‰¹å¤„ç†ï¼‰

    Args:
        temp_file: å¾…ç¿»è¯‘å•è¯çš„ä¸´æ—¶æ–‡ä»¶
        translation_file: ç¿»è¯‘åçš„ JSON æ–‡ä»¶
        output_file: è¾“å‡ºæ–‡ä»¶å
    """
    # åŠ è½½å¾…ç¿»è¯‘æ•°æ®
    uncached_data = json.loads(Path(temp_file).read_text(encoding='utf-8'))
    directory = uncached_data['directory']

    # åŠ è½½ç¿»è¯‘æ•°æ®
    translations = json.loads(Path(translation_file).read_text(encoding='utf-8'))

    # ç¡®ä¿æ˜¯åˆ—è¡¨
    if isinstance(translations, dict):
        translations = [translations]

    print(f"\n[4/5] ä¿å­˜ç¿»è¯‘åˆ°ç¼“å­˜")
    cache = TranslationCache()

    # åˆ›å»ºç¿»è¯‘å­—å…¸ï¼ˆå°å†™å•è¯ -> ç¿»è¯‘ï¼‰å¹¶ä¿å­˜åˆ°ç¼“å­˜
    translated_count = 0
    for trans in translations:
        word_lower = trans['word'].lower()

        # ä¿å­˜åˆ°ç¼“å­˜
        cache.add(
            word=trans['word'],
            translation=trans['translation'],
            sentence=trans.get('sentence', ''),
            sentence_translation=trans['sentence_translation']
        )
        translated_count += 1

    print(f"  âœ“ å·²ä¿å­˜ {translated_count} ä¸ªç¿»è¯‘åˆ°ç¼“å­˜")

    # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹æ¬¡å¤„ç†
    batch_info = uncached_data.get('batch_info', '')
    if batch_info:
        print(f"\n  å½“å‰æ‰¹æ¬¡ï¼š{batch_info}")

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®Œæˆ
        dir_name = Path(directory).name
        check_and_merge_batches(directory, dir_name, output_file)
    else:
        # å•æ‰¹å¤„ç†ï¼Œç›´æ¥ç”Ÿæˆ Anki æ–‡ä»¶
        print("\n[5/5] é‡æ–°æå–å¹¶ç”Ÿæˆ Anki æ–‡ä»¶")
        all_data = extract_words_from_directory(directory, '/tmp')

        # å¡«å……æ‰€æœ‰ç¿»è¯‘
        for file_data in all_data:
            for word_item in file_data['words']:
                word = word_item['word_lower']
                cached_translation = cache.get(word)
                if cached_translation:
                    word_item['translation'] = cached_translation['translation']
                    examples = cached_translation.get('sentence_examples', [])
                    if examples:
                        word_item['sentence_translation'] = examples[0]['sentence_translation']

        # ç”Ÿæˆ Anki æ–‡ä»¶
        if output_file is None:
            dir_name = Path(directory).name
            output_dir = get_output_dir()
            output_file = str(output_dir / f"{dir_name}.txt")

        generate_anki_tsv(all_data, output_file, deck_name=dir_name)
        print(f"  âœ“ å·²ç”Ÿæˆï¼š{output_file}")

        print("\nå®Œæˆï¼")


def check_and_merge_batches(directory: str, dir_name: str, output_file: str = None):
    """
    æ£€æŸ¥æ‰€æœ‰æ‰¹æ¬¡æ˜¯å¦éƒ½å·²ç¿»è¯‘å®Œæˆï¼Œå¦‚æœæ˜¯åˆ™åˆå¹¶ç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶

    Args:
        directory: æºç›®å½•
        dir_name: ç›®å½•åç§°
        output_file: è¾“å‡ºæ–‡ä»¶å
    """
    # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„æ‰¹æ¬¡æ–‡ä»¶
    tmp_dir = Path('/tmp')
    batch_files = sorted(tmp_dir.glob(f"{dir_name}_to_translate_batch_*.json"))

    if not batch_files:
        print("  âš ï¸  æœªæ‰¾åˆ°å…¶ä»–æ‰¹æ¬¡æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯å•æ‰¹å¤„ç†")
        return

    total_batches = len(batch_files)
    print(f"\n  æ£€æµ‹åˆ° {total_batches} ä¸ªæ‰¹æ¬¡æ–‡ä»¶")

    # è¯»å–æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦éƒ½å·²ç¿»è¯‘
    all_uncached_words = []
    untranslated_batches = []
    cache = TranslationCache()

    for batch_file in batch_files:
        batch_data = json.loads(batch_file.read_text(encoding='utf-8'))
        batch_num = batch_data.get('batch_info', '').split('/')[0].split()[-1]

        # æ£€æŸ¥è¿™æ‰¹å•è¯æ˜¯å¦éƒ½å·²ç¿»è¯‘ï¼ˆæ˜¯å¦åœ¨ç¼“å­˜ä¸­ï¼‰
        all_translated = True

        for word_item in batch_data['words']:
            word = word_item['word_lower']
            cached_translation = cache.get(word)

            if not cached_translation:
                all_translated = False
                untranslated_batches.append(batch_num)
                break

        if not all_translated:
            # è¿™æ‰¹è¿˜æœ‰æœªç¿»è¯‘çš„å•è¯
            pass
        else:
            # è¿™æ‰¹å·²å…¨éƒ¨ç¿»è¯‘
            all_uncached_words.extend(batch_data['words'])

    # å¦‚æœè¿˜æœ‰æœªç¿»è¯‘çš„æ‰¹æ¬¡ï¼Œæç¤ºç”¨æˆ·ç»§ç»­
    if untranslated_batches:
        print(f"\n  âš ï¸  è¿˜æœ‰ {len(untranslated_batches)} ä¸ªæ‰¹æ¬¡æœªå®Œæˆç¿»è¯‘ï¼šæ‰¹æ¬¡ {', '.join(untranslated_batches)}")
        print("\n  è¯·ç»§ç»­ç¿»è¯‘å‰©ä½™æ‰¹æ¬¡ï¼Œç„¶åè¿è¡Œå¯¹åº”çš„ä¿å­˜å‘½ä»¤")
        return

    # æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶
    print(f"\n  âœ“ æ‰€æœ‰ {total_batches} ä¸ªæ‰¹æ¬¡éƒ½å·²å®Œæˆç¿»è¯‘")
    print("\n[5/5] é‡æ–°æå–å¹¶ç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶")

    all_data = extract_words_from_directory(directory, '/tmp')

    # å¡«å……æ‰€æœ‰ç¿»è¯‘
    for file_data in all_data:
        for word_item in file_data['words']:
            word = word_item['word_lower']
            cached_translation = cache.get(word)
            if cached_translation:
                word_item['translation'] = cached_translation['translation']
                examples = cached_translation.get('sentence_examples', [])
                if examples:
                    word_item['sentence_translation'] = examples[0]['sentence_translation']

    # ç”Ÿæˆ Anki æ–‡ä»¶
    if output_file is None:
        output_dir = get_output_dir()
        output_file = str(output_dir / f"{dir_name}.txt")

    generate_anki_tsv(all_data, output_file, deck_name=dir_name)
    print(f"  âœ“ å·²ç”Ÿæˆï¼š{output_file}")

    print("\nå®Œæˆï¼æ‰€æœ‰æ‰¹æ¬¡å·²åˆå¹¶å¹¶ç”Ÿæˆ Anki æ–‡ä»¶")

    # æç¤ºæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    print(f"\nğŸ’¡ æç¤ºï¼šä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶ä½äº /tmp/{dir_name}_to_translate_batch_*.json")
    print("  å¯ä»¥æ‰‹åŠ¨åˆ é™¤è¿™äº›ä¸´æ—¶æ–‡ä»¶")


def main():
    if len(sys.argv) < 2:
        print("Usage:")
        print("  # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡æå–ç”Ÿè¯å¹¶æŸ¥è¯¢ç¼“å­˜")
        print("  python3 process_directory.py <directory>")
        print()
        print("  # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Claude Code ç¿»è¯‘æ¯æ‰¹å•è¯å¹¶ä¿å­˜")
        print("  python3 process_directory.py <directory> <translation.json> [output.txt]")
        print()
        print("è¯´æ˜ï¼š")
        print("  - å½“å•è¯æ•°é‡ â‰¤ 30 æ—¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–‡ä»¶ï¼Œç¿»è¯‘åç›´æ¥ç”Ÿæˆ Anki æ–‡ä»¶")
        print("  - å½“å•è¯æ•°é‡ > 30 æ—¶ï¼Œåˆ†æ‰¹ç”Ÿæˆå¤šä¸ªæ–‡ä»¶ï¼Œæ¯æ‰¹ç¿»è¯‘å®Œæˆåè‡ªåŠ¨æ£€æŸ¥")
        print("  - æ‰€æœ‰æ‰¹æ¬¡å®Œæˆåï¼Œè‡ªåŠ¨åˆå¹¶ç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶")
        sys.exit(1)

    directory = sys.argv[1]

    if not Path(directory).is_dir():
        print(f"Error: Directory not found: {directory}")
        sys.exit(1)

    if len(sys.argv) == 2:
        # ç¬¬ä¸€æ­¥ï¼šæå–å¹¶æŸ¥è¯¢ç¼“å­˜
        process_directory(directory)

    elif len(sys.argv) >= 3:
        # ç¬¬äºŒæ­¥ï¼šä¿å­˜ç¿»è¯‘å¹¶ç”Ÿæˆ
        translation_file = sys.argv[2]
        output_file = sys.argv[3] if len(sys.argv) > 3 else None

        if not Path(translation_file).exists():
            print(f"Error: Translation file not found: {translation_file}")
            sys.exit(1)

        # æŸ¥æ‰¾ä¸´æ—¶æ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹æ¬¡å’Œéæ‰¹æ¬¡ï¼‰
        dir_name = Path(directory).name
        temp_file = None

        # å…ˆå°è¯•æŸ¥æ‰¾æ‰¹æ¬¡æ–‡ä»¶ï¼ˆä»ç¬¬äºŒä¸ªå‚æ•°çš„æ–‡ä»¶åæ¨æ–­æ‰¹æ¬¡å·ï¼‰
        if 'batch' in translation_file.lower():
            import re
            batch_match = re.search(r'batch[_\s]*(\d+)', translation_file.lower())
            if batch_match:
                batch_num = batch_match.group(1)
                temp_file = Path('/tmp') / f"{dir_name}_to_translate_batch_{batch_num}.json"

        # å¦‚æœä¸æ˜¯æ‰¹æ¬¡æ–‡ä»¶æˆ–æœªæ‰¾åˆ°ï¼Œå°è¯•éæ‰¹æ¬¡æ–‡ä»¶
        if temp_file is None or not temp_file.exists():
            temp_file = Path('/tmp') / f"{dir_name}_to_translate.json"

        if not temp_file.exists():
            print(f"Error: Temp file not found: {temp_file}")
            print("è¯·å…ˆè¿è¡Œç¬¬ä¸€æ­¥ï¼špython3 process_directory.py <directory>")
            sys.exit(1)

        save_and_generate(str(temp_file), translation_file, output_file)


if __name__ == '__main__':
    main()
