#!/usr/bin/env python3
"""
å•æ–‡ä»¶å¤„ç†é›†æˆè„šæœ¬

è‡ªåŠ¨å¤„ç†å•ä¸ª Markdown æ–‡ä»¶ï¼ŒåŒ…æ‹¬ï¼š
1. æå–ç”Ÿè¯ï¼ˆæ–‡ä»¶å†…å»é‡ï¼‰
2. æŸ¥è¯¢ç¼“å­˜ï¼ˆå…¨å±€å»é‡ï¼‰
3. åˆ†æ‰¹è¾“å‡ºéœ€è¦ç¿»è¯‘çš„å•è¯ï¼ˆæ¯æ‰¹æœ€å¤š30ä¸ªï¼‰
4. ä½¿ç”¨ Claude Code ç¿»è¯‘æ¯æ‰¹å•è¯
5. ä¿å­˜ç¿»è¯‘åˆ°ç¼“å­˜
6. ç”Ÿæˆ Anki æ–‡ä»¶

ç¡®ä¿æ‰€æœ‰å•è¯åªç¿»è¯‘ä¸€æ¬¡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ã€‚
"""

import json
import sys
from pathlib import Path

# å¯¼å…¥å…¶ä»–æ¨¡å—
from extract_words import extract_words_from_file
from translation_cache import TranslationCache
from generate_anki import generate_anki_tsv
from config import get_output_dir

# æ¯æ‰¹ç¿»è¯‘çš„æœ€å¤§å•è¯æ•°
BATCH_SIZE = 30


def process_file(markdown_file: str, output_dir: str = None) -> dict:
    """
    å¤„ç†å•ä¸ª Markdown æ–‡ä»¶

    Args:
        markdown_file: Markdown æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•

    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    # 1. æå–ç”Ÿè¯ï¼ˆæ–‡ä»¶å†…å·²å»é‡ï¼‰
    print(f"[1/5] æå–ç”Ÿè¯ï¼š{markdown_file}")
    data = extract_words_from_file(markdown_file)
    total_words = data['word_count']
    print(f"  âœ“ æå–åˆ° {total_words} ä¸ªç”Ÿè¯ï¼ˆå·²å»é‡ï¼‰")

    if total_words == 0:
        print("  æ²¡æœ‰æ‰¾åˆ°æ ‡è®°çš„ç”Ÿè¯ï¼Œé€€å‡º")
        return {'total': 0, 'cached': 0, 'new': 0}

    # 2. æŸ¥è¯¢ç¼“å­˜ï¼ˆå…¨å±€å»é‡ï¼‰
    print("\n[2/5] æŸ¥è¯¢ç¿»è¯‘ç¼“å­˜")
    cache = TranslationCache()

    cached_words = []
    uncached_words = []

    for word_item in data['words']:
        word = word_item['word_lower']
        cached_translation = cache.get(word)

        if cached_translation:
            # ä½¿ç”¨ç¼“å­˜çš„ç¿»è¯‘
            word_item['translation'] = cached_translation['translation']
            # å¯¹äºä¾‹å¥ç¿»è¯‘ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„ç¬¬ä¸€ä¸ªä¾‹å¥
            examples = cached_translation.get('sentence_examples', [])
            if examples:
                word_item['sentence_translation'] = examples[0]['sentence_translation']
            else:
                word_item['sentence_translation'] = ''
            cached_words.append(word_item)
        else:
            uncached_words.append(word_item)

    print(f"  âœ“ æ‰¾åˆ° {len(cached_words)} ä¸ªå·²ç¼“å­˜çš„å•è¯")
    print(f"  âœ“ éœ€è¦ç¿»è¯‘ {len(uncached_words)} ä¸ªæ–°å•è¯")

    # 3. è¾“å‡ºéœ€è¦ç¿»è¯‘çš„å•è¯ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    if uncached_words:
        print("\n[3/5] éœ€è¦ç¿»è¯‘çš„å•è¯åˆ—è¡¨ï¼š")
        print("â”€" * 60)

        # è®¡ç®—éœ€è¦åˆ†æˆå¤šå°‘æ‰¹
        total_batches = (len(uncached_words) + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"\n  æ€»å…± {len(uncached_words)} ä¸ªæ–°å•è¯ï¼Œå°†åˆ†æˆ {total_batches} æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹æœ€å¤š {BATCH_SIZE} ä¸ªï¼‰")

        # åˆ†æ‰¹ä¿å­˜å¾…ç¿»è¯‘å•è¯åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_files = []
        for batch_num in range(total_batches):
            start_idx = batch_num * BATCH_SIZE
            end_idx = min((batch_num + 1) * BATCH_SIZE, len(uncached_words))
            batch_words = uncached_words[start_idx:end_idx]

            # ç”Ÿæˆæ‰¹æ¬¡æ–‡ä»¶å
            if total_batches == 1:
                temp_file = Path('/tmp') / f"{data['deck_name']}_to_translate.json"
            else:
                temp_file = Path('/tmp') / f"{data['deck_name']}_to_translate_batch_{batch_num + 1}.json"

            batch_data = {
                'deck_name': data['deck_name'],
                'batch_info': f"æ‰¹æ¬¡ {batch_num + 1}/{total_batches}",
                'words': batch_words
            }
            temp_file.write_text(
                json.dumps(batch_data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            temp_files.append(str(temp_file))

            print(f"\næ‰¹æ¬¡ {batch_num + 1}/{total_batches}ï¼š{len(batch_words)} ä¸ªå•è¯ -> {temp_file}")

        print("\n" + "â”€" * 60)
        print("\nğŸ“ ä½¿ç”¨ Claude Code ç¿»è¯‘å•è¯ï¼š")
        print("\nå¯¹äºæ¯ä¸ªæ‰¹æ¬¡æ–‡ä»¶ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š")
        print("\n1. åœ¨ Claude Code ä¸­è¾“å…¥ï¼š")
        print("   \"è¯·å¸®æˆ‘ç¿»è¯‘è¿™ä¸ªæ–‡ä»¶ä¸­çš„å•è¯ï¼ŒæŒ‰ç…§æ–‡ä»¶ä¸­æä¾›çš„æ ¼å¼è¿”å›ç¿»è¯‘ç»“æœ\"")
        print("\n2. å°†ç¿»è¯‘ç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼štranslation_batch_1.jsonï¼‰")
        print("\n3. è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¿å­˜ç¿»è¯‘ï¼š")
        for i, temp_file in enumerate(temp_files, 1):
            print(f"   python3 scripts/process_file.py {temp_file} translation_batch_{i}.json")

        print("\n" + "â”€" * 60)
        print("\nç¿»è¯‘æ ¼å¼ç¤ºä¾‹ï¼ˆéœ€åˆ—å‡ºå•è¯çš„æ‰€æœ‰å¸¸ç”¨è¯ä¹‰ï¼Œä¸è¯å…¸ä¸€è‡´ï¼‰ï¼š")
        print('[')
        print('  {')
        print('    "word": "example",')
        print('    "translation": "n. ä¾‹å­ï¼›èŒƒä¾‹ï¼›æ¦œæ · v. ä½œä¸º...çš„ä¾‹å­",')
        print('    "sentence": "This is an example.",')
        print('    "sentence_translation": "è¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚"')
        print('  }')
        print(']')

        print("\nğŸ’¡ æç¤ºï¼š")
        print(f"  - æ¯æ‰¹æœ€å¤š {BATCH_SIZE} ä¸ªå•è¯ï¼Œç¡®ä¿ä¸è¶…è¿‡ Claude Code çš„ä¸Šä¸‹æ–‡é™åˆ¶")
        print("  - ç¿»è¯‘å®Œä¸€æ‰¹åå†å¤„ç†ä¸‹ä¸€æ‰¹")
        print("  - æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆåï¼Œä¼šè‡ªåŠ¨åˆå¹¶ç”Ÿæˆæœ€ç»ˆçš„ Anki æ–‡ä»¶")

        return {
            'total': total_words,
            'cached': len(cached_words),
            'new': len(uncached_words),
            'temp_files': temp_files,
            'total_batches': total_batches
        }

    else:
        print("\n[3/5] æ‰€æœ‰å•è¯éƒ½å·²ç¼“å­˜ï¼Œæ— éœ€ç¿»è¯‘")

        # 4. ç”Ÿæˆ Anki æ–‡ä»¶
        print("\n[4/5] ç”Ÿæˆ Anki æ–‡ä»¶")
        data['words'] = cached_words

        if output_dir is None:
            output_dir = get_output_dir()
        else:
            output_dir = Path(output_dir)

        output_file = output_dir / f"{data['deck_name']}.txt"
        generate_anki_tsv(data, str(output_file))
        print(f"  âœ“ å·²ç”Ÿæˆï¼š{output_file}")

        print("\n[5/5] å®Œæˆï¼")
        print(f"  æ€»å•è¯æ•°ï¼š{total_words}")
        print(f"  ä½¿ç”¨ç¼“å­˜ï¼š{len(cached_words)}")
        print(f"  æ–°ç¿»è¯‘ï¼š0")

        return {
            'total': total_words,
            'cached': len(cached_words),
            'new': 0,
            'output_file': str(output_file)
        }


def save_and_generate(temp_file: str, translation_file: str, output_dir: str = None):
    """
    ä¿å­˜ç¿»è¯‘å¹¶ç”Ÿæˆ Anki æ–‡ä»¶ï¼ˆæ”¯æŒå•æ‰¹å’Œå¤šæ‰¹å¤„ç†ï¼‰

    Args:
        temp_file: å¾…ç¿»è¯‘å•è¯çš„ä¸´æ—¶æ–‡ä»¶
        translation_file: ç¿»è¯‘åçš„ JSON æ–‡ä»¶
        output_dir: è¾“å‡ºç›®å½•
    """
    # åŠ è½½å¾…ç¿»è¯‘æ•°æ®
    uncached_data = json.loads(Path(temp_file).read_text(encoding='utf-8'))

    # åŠ è½½ç¿»è¯‘æ•°æ®
    translations = json.loads(Path(translation_file).read_text(encoding='utf-8'))

    # ç¡®ä¿æ˜¯åˆ—è¡¨
    if isinstance(translations, dict):
        translations = [translations]

    print(f"\n[4/5] ä¿å­˜ç¿»è¯‘åˆ°ç¼“å­˜")
    cache = TranslationCache()

    # æ›´æ–°æœªç¼“å­˜å•è¯çš„ç¿»è¯‘
    translated_count = 0
    for word_item in uncached_data['words']:
        word = word_item['word_lower']

        # åœ¨ç¿»è¯‘æ•°æ®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ç¿»è¯‘
        for trans in translations:
            if trans['word'].lower() == word:
                word_item['translation'] = trans['translation']
                word_item['sentence_translation'] = trans['sentence_translation']

                # ä¿å­˜åˆ°ç¼“å­˜
                cache.add(
                    word=trans['word'],
                    translation=trans['translation'],
                    sentence=trans.get('sentence', word_item['sentence']),
                    sentence_translation=trans['sentence_translation']
                )
                translated_count += 1
                break

    print(f"  âœ“ å·²ä¿å­˜ {translated_count} ä¸ªç¿»è¯‘åˆ°ç¼“å­˜")

    # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹æ¬¡å¤„ç†
    batch_info = uncached_data.get('batch_info', '')
    if batch_info:
        print(f"\n  å½“å‰æ‰¹æ¬¡ï¼š{batch_info}")

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®Œæˆ
        deck_name = uncached_data['deck_name']
        check_and_merge_batches(deck_name, output_dir)
    else:
        # å•æ‰¹å¤„ç†ï¼Œç›´æ¥ç”Ÿæˆ Anki æ–‡ä»¶
        print("\n[5/5] ç”Ÿæˆ Anki æ–‡ä»¶")

        if output_dir is None:
            output_dir = get_output_dir()
        else:
            output_dir = Path(output_dir)

        output_file = output_dir / f"{uncached_data['deck_name']}.txt"
        generate_anki_tsv(uncached_data, str(output_file))
        print(f"  âœ“ å·²ç”Ÿæˆï¼š{output_file}")

        print("\nå®Œæˆï¼")


def check_and_merge_batches(deck_name: str, output_dir: str = None):
    """
    æ£€æŸ¥æ‰€æœ‰æ‰¹æ¬¡æ˜¯å¦éƒ½å·²ç¿»è¯‘å®Œæˆï¼Œå¦‚æœæ˜¯åˆ™åˆå¹¶ç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶

    Args:
        deck_name: ç‰Œç»„åç§°
        output_dir: è¾“å‡ºç›®å½•
    """
    # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„æ‰¹æ¬¡æ–‡ä»¶
    tmp_dir = Path('/tmp')
    batch_files = sorted(tmp_dir.glob(f"{deck_name}_to_translate_batch_*.json"))

    if not batch_files:
        print("  âš ï¸  æœªæ‰¾åˆ°å…¶ä»–æ‰¹æ¬¡æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯å•æ‰¹å¤„ç†")
        return

    total_batches = len(batch_files)
    print(f"\n  æ£€æµ‹åˆ° {total_batches} ä¸ªæ‰¹æ¬¡æ–‡ä»¶")

    # è¯»å–æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦éƒ½å·²ç¿»è¯‘
    all_words = []
    untranslated_batches = []

    for batch_file in batch_files:
        batch_data = json.loads(batch_file.read_text(encoding='utf-8'))
        batch_num = batch_data.get('batch_info', '').split('/')[0].split()[-1]

        # æ£€æŸ¥è¿™æ‰¹å•è¯æ˜¯å¦éƒ½å·²ç¿»è¯‘ï¼ˆæ˜¯å¦åœ¨ç¼“å­˜ä¸­ï¼‰
        cache = TranslationCache()
        all_translated = True

        for word_item in batch_data['words']:
            word = word_item['word_lower']
            cached_translation = cache.get(word)

            if cached_translation:
                # ä½¿ç”¨ç¼“å­˜çš„ç¿»è¯‘
                word_item['translation'] = cached_translation['translation']
                examples = cached_translation.get('sentence_examples', [])
                if examples:
                    word_item['sentence_translation'] = examples[0]['sentence_translation']
            else:
                all_translated = False
                untranslated_batches.append(batch_num)
                break

        if all_translated:
            all_words.extend(batch_data['words'])

    # å¦‚æœè¿˜æœ‰æœªç¿»è¯‘çš„æ‰¹æ¬¡ï¼Œæç¤ºç”¨æˆ·ç»§ç»­
    if untranslated_batches:
        print(f"\n  âš ï¸  è¿˜æœ‰ {len(untranslated_batches)} ä¸ªæ‰¹æ¬¡æœªå®Œæˆç¿»è¯‘ï¼šæ‰¹æ¬¡ {', '.join(untranslated_batches)}")
        print("\n  è¯·ç»§ç»­ç¿»è¯‘å‰©ä½™æ‰¹æ¬¡ï¼Œç„¶åè¿è¡Œå¯¹åº”çš„ä¿å­˜å‘½ä»¤")
        return

    # æ‰€æœ‰æ‰¹æ¬¡éƒ½å·²å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶
    print(f"\n  âœ“ æ‰€æœ‰ {total_batches} ä¸ªæ‰¹æ¬¡éƒ½å·²å®Œæˆç¿»è¯‘")
    print("\n[5/5] ç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶")

    if output_dir is None:
        output_dir = get_output_dir()
    else:
        output_dir = Path(output_dir)

    final_data = {
        'deck_name': deck_name,
        'words': all_words
    }

    output_file = output_dir / f"{deck_name}.txt"
    generate_anki_tsv(final_data, str(output_file))
    print(f"  âœ“ å·²ç”Ÿæˆï¼š{output_file}")

    print("\nå®Œæˆï¼æ‰€æœ‰æ‰¹æ¬¡å·²åˆå¹¶å¹¶ç”Ÿæˆ Anki æ–‡ä»¶")
    print(f"  æ€»å•è¯æ•°ï¼š{len(all_words)}")

    # è¯¢é—®æ˜¯å¦åˆ é™¤ä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶
    print(f"\nğŸ’¡ æç¤ºï¼šä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶ä½äº /tmp/{deck_name}_to_translate_batch_*.json")
    print("  å¯ä»¥æ‰‹åŠ¨åˆ é™¤è¿™äº›ä¸´æ—¶æ–‡ä»¶")


def main():
    if len(sys.argv) < 2:
        print("Usage:")
        print("  # ç¬¬ä¸€æ­¥ï¼šæå–ç”Ÿè¯å¹¶æŸ¥è¯¢ç¼“å­˜")
        print("  python3 process_file.py <markdown_file> [output_dir]")
        print()
        print("  # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Claude Code ç¿»è¯‘æ¯æ‰¹å•è¯å¹¶ä¿å­˜")
        print("  python3 process_file.py <batch_file> <translation.json> [output_dir]")
        print()
        print("è¯´æ˜ï¼š")
        print("  - å½“å•è¯æ•°é‡ â‰¤ 30 æ—¶ï¼Œç”Ÿæˆä¸€ä¸ªæ–‡ä»¶ï¼Œç¿»è¯‘åç›´æ¥ç”Ÿæˆ Anki æ–‡ä»¶")
        print("  - å½“å•è¯æ•°é‡ > 30 æ—¶ï¼Œåˆ†æ‰¹ç”Ÿæˆå¤šä¸ªæ–‡ä»¶ï¼Œæ¯æ‰¹ç¿»è¯‘å®Œæˆåè‡ªåŠ¨æ£€æŸ¥")
        print("  - æ‰€æœ‰æ‰¹æ¬¡å®Œæˆåï¼Œè‡ªåŠ¨åˆå¹¶ç”Ÿæˆæœ€ç»ˆ Anki æ–‡ä»¶")
        print("  - output_dir ä¸ºå¯é€‰å‚æ•°ï¼ŒæŒ‡å®š Anki æ–‡ä»¶çš„è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰")
        sys.exit(1)

    if len(sys.argv) == 2 or (len(sys.argv) == 3 and not sys.argv[2].endswith('.json')):
        # ç¬¬ä¸€æ­¥ï¼šæå–å¹¶æŸ¥è¯¢ç¼“å­˜
        markdown_file = sys.argv[1]
        output_dir = sys.argv[2] if len(sys.argv) > 2 else None

        if not Path(markdown_file).exists():
            print(f"Error: File not found: {markdown_file}")
            sys.exit(1)

        process_file(markdown_file, output_dir)

    elif len(sys.argv) >= 3:
        # ç¬¬äºŒæ­¥ï¼šä¿å­˜ç¿»è¯‘å¹¶ç”Ÿæˆ
        temp_file = sys.argv[1]
        translation_file = sys.argv[2]
        output_dir = sys.argv[3] if len(sys.argv) > 3 else None

        if not Path(temp_file).exists():
            print(f"Error: Temp file not found: {temp_file}")
            sys.exit(1)

        if not Path(translation_file).exists():
            print(f"Error: Translation file not found: {translation_file}")
            sys.exit(1)

        save_and_generate(temp_file, translation_file, output_dir)


if __name__ == '__main__':
    main()
